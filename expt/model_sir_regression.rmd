---
title:  "SIR Regression Model"
author: "Tom Shafer"
date:   "2020-06-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
options(stringsAsFactors = FALSE)

suppressPackageStartupMessages({
  library(magrittr)
  library(tidyverse)
  library(here)
  library(brms)
  library(tidybayes)
  library(furrr)
  library(widyr)
})

source(here("src/R/trent/R/sir.R"))
```


# Overview

## Objective

Fit SIR models through time, preferably using BRMS, to estimate the effects of various interventions.

## Theory

Converting the equation for $\dot I(t)$ to a difference equation gives
$$
I(T+1) = I(T) \cdot \left[ (1 - \gamma) + \beta S(T) / N \right],
$$
with $N$ the population size. Modeling this directly imposes a number of assumptions, including homogeneous mixing and that a county's population is fixed within the county. In practice this second assumptions shouldn't matter too much because the _effective_ community size for an individual is (likely?) much smaller than their entire county. 

## Assumptions

 - We can fit the SIR equation directly using raw infections data
 - I wonder if we can scale $I(t)$ and see how it goes.
 - Very simple imputations are used here

## Results

Adding confounders doesn't seem to hurt the R0 estimate, and the coefficients look OK directionally. I've also tried 3-way interactions to try and capture $d\beta/dX$, but it isn't clear that is the correct thing.

I'm also very aware that I'm getting the numbers I _want_ to get (positive R0 > 1) so I need to be even more suspicious of this.


# Data prep

## Loading and checking

This function puts the data in SIR form, meaning:

 - *I(t)* is calculated by differencing `confirmed`
 - *S(t)* is calculated directly from `confirmed` and the ACS population measure

We've also found that some $I(t)$ and $\mathbb{I}(T) \equiv \sum_{t=0}^T I(t)$ entries are negative, maybe because of corrections to the data made later. So we apply an approach that:

 1. Corrects $I(t) \to 0$ if negative
 2. Back-corrects $\mathbb{I}(t-1)$ and $I(t-1)$ to make things work out OK


```{r}
abt_master <- fetch_abt(here("data/processed/ABT_V1.csv"))
```

No missing values:

```{r, echo = F}
abt_master %>% 
  summarize(across(.fn = ~ sum(is.na(.)))) %>% 
  gather(column, na_values) %>% 
  knitr::kable()
```

Correlations:

```{r, echo = F}
abt_master %>% 
  select(where(is.numeric)) %>% 
  cor() %>% 
  corrplot::corrplot(method = "color", order = "hclust", tl.col = "black")
```


# Modeling

## Data review

How are the various quantities distributed?
This will be challenging to fit.

```{r, echo = F}
abt_master %>% 
  ggplot(aes(cuml_inf)) +
  geom_histogram(bins = 20)

abt_master %>% 
  ggplot(aes(daily_inf)) +
  geom_histogram(bins = 20)

abt_master %>% 
  ggplot(aes(suscept_norm)) +
  geom_histogram(bins = 20)
```

## Simple linear regression

Fit simple linear regressions at various levels to convince myself this is worth pursuing further.

```{r}
# Simple, SIR-based fit
simple_lm_fit <- function(data) {
  lm(
    daily_inf_target ~ daily_inf + daily_inf:suscept_norm, 
    data = data, 
    weights = data$pop
  )
}

# Add some potential confounders
less_simple_lm_fit <- function(data) {
  lm(daily_inf_target ~ 
       daily_inf + daily_inf:suscept_norm
     + median_inc + minority_frac + tmpf_mean + relh_mean
     + mobility_transit_stations + mobility_retail_and_recreation 
     + mobility_grocery_and_pharmacy + mobility_parks
     + mobility_workplaces + mobility_residential
     + phase_1_active + phase_1_ever, data = data, weights=data$pop)
}
```

### Global model

The global model is very confident of the parameters:

```{r}
lm_all_global <- abt_master %>% 
  summarize(model = list(simple_lm_fit(.))) %>% 
  extract_lm(model, which = "coef")

lm_all_global %>% 
  knitr::kable()

# R0 = beta / gamma
lm_all_global[[3, 3]] / (1 - lm_all_global[[2, 3]])
```

The confounder model is plausible (not changing R0 by much). And I've done no feature selection, no scaling, etc.

```{r}
lm_all_global_confound <- abt_master %>% 
  summarize(model = list(less_simple_lm_fit(.))) %>% 
  extract_lm(model, which = "coef")

lm_all_global_confound %>% 
  knitr::kable()

# R0 = beta / gamma
lm_all_global_confound[[15, 3]] / (1 - lm_all_global[[2, 3]])
```

Let's try a more physically interesting model.

```{r}
lm_fit_3 <- function(data) {
  lm(daily_inf_target ~ daily_inf + daily_inf:suscept_norm
     + daily_inf:suscept_norm:phase_1_active
     + daily_inf:suscept_norm:mobility_transit_stations
     + daily_inf:suscept_norm:minority_frac
     + daily_inf:phase_1_active
     + daily_inf:mobility_transit_stations
     + daily_inf:minority_frac
     + median_inc + minority_frac + tmpf_mean + relh_mean
     + mobility_transit_stations + mobility_retail_and_recreation 
     + mobility_grocery_and_pharmacy + mobility_parks
     + mobility_workplaces + mobility_residential
     + phase_1_active + phase_1_ever, data = data, weights = data$pop)
}
```

```{r}
lm_all_global_confound_3 <- abt_master %>% 
  summarize(model = list(lm_fit_3(.))) %>% 
  extract_lm(model, which = "coef")

lm_all_global_confound_3 %>%
  knitr::kable()

lm_all_global_confound_3 %>% 
  summarize(
    beta = estimate[term == "daily_inf:suscept_norm"], 
    gamma = 1 - estimate[term == "daily_inf"], 
    r0 = beta / gamma
  ) %>% 
  arrange(-r0)
```


### State model

```{r}
lm_all_state <- abt_master %>% 
  group_by(state_code) %>% 
  group_modify(~ {
    .x %>% 
      summarize(model = list(simple_lm_fit(.))) %>% 
      extract_lm(model, which = "coef")
  }) %>% 
  ungroup()

lm_all_state %>% 
  knitr::kable()

# One wonders what drives the issues with beta and gamma.
# That we get decent R0 values suggests it's a measurment issue for I(t) ?
lm_all_state %>% 
  group_by(state_code) %>% 
  summarize(beta = estimate[3], gamma = 1 - estimate[2], r0 = beta / gamma) %>% 
  arrange(-r0)
```

Again, with confounders: still OK.

```{r}
lm_all_state_confound <- abt_master %>% 
  group_by(state_code) %>% 
  group_modify(~ {
    .x %>% 
      summarize(model = list(less_simple_lm_fit(.))) %>% 
      extract_lm(model, which = "coef")
  }) %>% 
  ungroup()

lm_all_state_confound %>% 
  knitr::kable()

lm_all_state_confound %>% 
  group_by(state_code) %>% 
  summarize(
    beta = estimate[term == "daily_inf:suscept_norm"], 
    gamma = 1 - estimate[term == "daily_inf"], 
    r0 = beta / gamma
  ) %>% 
  arrange(-r0)
```

### State model with smoothing

We're all pretty sure that $I(t)$ is underestimated, right? What if we "manually" change $I(t)$ by some percentage and re-fit? (We aren't being careful here; in some places we're certainly saying there are _more_ infections than people, so take large values with a grain of salt.)

```{r}
REFIT_MULTIPLIERS <- c(1.1, 1.2, 1.5, 2, 5, 10)

smoothing <- map_df(set_names(REFIT_MULTIPLIERS), ~ {
  mm <- .x
  message("Refit parameter: ", mm)
  
  abt_master %>% 
    group_arrange_abt() %T>% 
    {message("Adjusting the ABT")} %>% 
    mutate(across(c(cuml_inf, daily_inf, daily_inf_target), ~ . * mm)) %>% 
    mutate(suscept_norm = 1 - cuml_inf / pop) %>% 
    group_by(state_code) %T>% 
    {message("Modeling")} %>% 
    group_modify(~ {
      .x %>% 
        summarize(model = list(less_simple_lm_fit(.))) %>% 
        extract_lm(model)
    }) %>% 
    ungroup()
}, .id = "refit_mult") %>% 
  mutate(refit_mult = as.double(refit_mult))
```

It seems that, as I expected, R0 stays pretty consistent-ish as $\beta$ and $\gamma$ move in union. The error bars go downward, too.

```{r, echo = F}
smoothing %>% 
  group_by(refit_mult, state_code) %>% 
  summarize(
    beta = estimate[term == "daily_inf:suscept_norm"], 
    beta_err = std.error[term == "daily_inf:suscept_norm"], 
    gamma = 1 - estimate[term == "daily_inf"], 
    gamma_err = std.error[term == "daily_inf"], 
    # r2 = estimate[term == "AIC"], 
    r0 = beta / gamma
  ) %>% 
  ungroup() %>% 
  gather(k, v, beta:r0) %>% 
  ggplot(aes(refit_mult, v, color = state_code, group = state_code)) + 
  geom_line() + 
  facet_wrap(~ k, scales = "free_y") + 
  scale_color_discrete(guide = F)
```

What do the variable distributions look like when smoothed? Are they easier to fit? Maybe a _very_ little bit.

```{r, echo=F}
abt_master %>% 
  group_arrange_abt() %>% 
  mutate(across(c(cuml_inf, daily_inf, daily_inf_target), ~ . * 2)) %>% 
  mutate(suscept_norm = 1 - cuml_inf / pop) %>% 
  ungroup() %>% 
  select(cuml_inf, daily_inf, daily_inf_target, suscept_norm) %>% 
  gather(k, v) %>% 
  ggplot(aes(v)) + 
  geom_histogram() + 
  facet_wrap(~ k, scales = "free")
```


### County model

The county model can be...less confident. And wilder.

```{r}
lm_all_county <- abt_master %>% 
  group_by(state_code, county_fip) %>% 
  group_modify(~ {
    .x %>% 
      summarize(model = list(simple_lm_fit(.))) %>% 
      extract_lm(model, which = "coef")
  }) %>% 
  ungroup()

lm_all_county %>% 
  group_by(state_code, county_fip) %>% 
  summarize(beta = estimate[3], gamma = 1 - estimate[2], r0 = beta / gamma) %>% 
  arrange(-r0)
```

This is a very interesting plot...

```{r, echo = F}
abt_master %>% 
  distinct(county_fip, pop) %>% 
  left_join(lm_all_county, ., by = "county_fip") %>% 
  filter(str_detect(term, "daily_inf")) %>% 
  ggplot() + 
  geom_point(aes(pop, estimate)) + 
  geom_errorbar(aes(
    pop,
    ymin = estimate - 2 * std.error,
    ymax = estimate + 2 * std.error
  )) +
  facet_wrap(~ term) + 
  scale_x_log10()
```

This suggests the model will be very tough to fit. Earlier plots suggested that $S(t) / N$ is so nearly 1.0 that it's tough to fit.

```{r}
lm_all_state %>% 
  filter(str_detect(term, "daily_inf")) %>%
  pairwise_cor(term, state_code, estimate)

lm_all_county %>% 
  filter(str_detect(term, "daily_inf")) %>%
  pairwise_cor(term, county_fip, estimate)
```

## LME4

This is completely unrigorous, but throw what we have from above into LME4 with random intercepts and see what we get. (Need to rescale, at a minimum.)

```{r}
mm <- lme4::lmer(
  daily_inf_target ~ 
    + daily_inf + daily_inf:suscept_norm
    # + daily_inf:suscept_norm:phase_1_active
     # + daily_inf:suscept_norm:mobility_transit_stations
     # + daily_inf:suscept_norm:minority_frac
     # + daily_inf:phase_1_active
     # + daily_inf:mobility_transit_stations
     # + daily_inf:minority_frac
     + median_inc + minority_frac + tmpf_mean + relh_mean
     + mobility_transit_stations + mobility_retail_and_recreation 
     + mobility_grocery_and_pharmacy + mobility_parks
     + mobility_workplaces + mobility_residential
     + phase_1_active + phase_1_ever +
    (1 | state_code/county_fip), 
  data = abt_master, 
  weights = abt_master$pop
)

summary(mm)
```
