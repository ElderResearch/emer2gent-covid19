---
title: "ABT Refine"
author: "Carlos Blancarte"
date: "Created: 2020-06-07, Updated: `r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, echo=FALSE}
r_root <- rprojroot::find_rstudio_root_file()

knitr::opts_knit$set(root.dir = file.path(r_root))
knitr::opts_chunk$set(warning = FALSE, message = FALSE, dpi = 100)
knitr::opts_chunk$set(fig.width = 7, fig.height = 5)

ggplot2::theme_set(ggplot2::theme_minimal())
```

```{r, lib-load}
suppressPackageStartupMessages({
  library(magrittr)
  library(dplyr)
  library(feather)
  library(forcats)
  library(fs)
  library(ggplot2)
  library(glue)
  library(lubridate)
  library(purrr)
  library(readr)
  library(rlang)
  library(stringr)
  library(tibble)
  library(tidyr)
  library(vroom)
  library(zoo)
})
```

# Load Data

```{r}
# determine absolute path to the overall **project** data directory
proj_data_path <- path_abs(path("..", "..", "..", 'data'))

# Use vroom and match read.csv introduction of "." in place of " "
d <- vroom::vroom(list.files(proj_data_path, pattern = 'ABT', full.names=TRUE)) %>% 
  rename_with(~ str_replace_all(., " ", ".")) %>%
  rename_all(tolower) %>%
  rename_all(~ gsub('\\.', '_', .)) %>%
  rename(
    cov_pos_tests = cov_positive,
    cov_neg_tests = cov_negative,
    cov_in_hosp = cov_hospitalizedcurrently,
    cov_total_pos = cov_total,
    cov_total_tests = cov_totaltestresults
)

d <- as_tibble(d) %>%
  mutate(date = as.Date(date)) %>%
  mutate(across(where(is.factor), as.character)) %>% 
  arrange(state, county, date)

```

# Convenience Functions

```{r}
correct_negative_it <- function(col) {
  # ex: c(1, 4, 6, 9, 2, 12, 15) # fix the 2
    new_col <- c(col)
    # Iterate backwards to index 2 (leave 1 out for calculating difference)
    for (i in seq(length(new_col), 2, -1)) {
        it <- new_col[i] - new_col[i - 1]
        if (it >= 0) next
        # Make it go to zero by fixing the last record
        new_col[i - 1] <- new_col[i - 1] + it
    }
    new_col
}
```

---

# Sanity Check

We know there are some issues with the reporting of cumulative confirmed 
cases - namely, a few counties mysteriously report _less_ cases than a
previous date.  

These will be corrected by replacing the errant value with the last known value.

```{r}
d %>% 
  arrange(state, county, date) %>% 
  group_by(state, county) %>% 
  filter(max(confirmed) != confirmed[max(row_number())]) %>% 
  qplot(date, confirmed, data = .) +
  facet_wrap(~ state + county, scales = 'free_y') + 
  ggtitle("Cumulative Cases with No Adjustment")

d %>% 
  arrange(state, county, date) %>% 
  group_by(state, county) %>% 
  filter(max(confirmed) != confirmed[max(row_number())]) %>% 
  mutate(confirmed2 = correct_negative_it(confirmed)) %>%
  qplot(date, confirmed2, data = .) +
  facet_wrap(~ state + county, scales = 'free_y') + 
  ggtitle("Cumulative Cases with Adjustment")
```

## Total Confirmed Infections

Confirm the total number of confirmed COVID-19 cases in the US (at least, according
to the data we've collected). As of June 6 we should be near the 2 million mark.

```{r}
d %>% 
  group_by(state, county) %>% 
  mutate(confirmed = correct_negative_it(confirmed)) %>%
  filter(row_number() == max(row_number())) %>% 
  ungroup() %>% 
  summarise(as_of = max(date), total_confirmed = scales::comma(sum(confirmed)))
```

## Number of Counties with No Confirmed Cases and Deaths

We expect a percentage of counties to have reported zero cases. This turns out
to be a case with approx 5% of all counties.  

```{r}
group_by(d, state, county) %>% 
  summarise(has_cases = max(confirmed) > 0,
            has_deaths = max(deaths) > 0,
            .groups = 'drop') %>% 
  group_by(has_cases, has_deaths) %>% 
  summarise(count = n(), .groups = 'drop') %>% 
  mutate(percent = scales::percent(count/sum(count)))
```

NB: We have a county with a death but no cases:

```{r}
d %>% 
  group_by(state, county) %>% 
  summarize(across(c(confirmed, deaths), max), .groups = "drop") %>% 
  filter(confirmed == 0, deaths > 0)
```

---

# Processing

## Correct the cumulative counts

```{r}
dd <- d %>% 
  group_by(state, county) %>% 
  mutate(across(c(confirmed, deaths), ~ correct_negative_it(.x))) %>%
  ungroup()
```

---

## Policy

The primary area of interest in estimating the impact of lifting restrictions
on the infection rate. Create a feature that maps the time that each county
spent without lock-down orders, with lock-down orders, and phased reopening.

```{r}
policy_cols <- c('stay_home', 'phase_1', 'phase_2', 'phase_3')
policy_labels <- c('none_issued', 'stay_home', 'phase_1', 'phase_2', 'phase_3') 

policies <- select(dd, state, county, county_fip, date, all_of(policy_cols)) %>% 
  mutate(across(all_of(policy_cols), ~ gsub('None Issued', 'none_issued', .x))) %>%
  mutate(across(all_of(policy_cols[-1]), ~ ifelse(.x=='1' & stay_home=='none_issued', NA, .x))) %>% 
  mutate(across(all_of(policy_cols), ~ na_if(.x, '0'))) %>%
  imap_dfc(~ if (.y %in% policy_cols) {if_else(.x == '1', .y, .x)} else {.x}) %>% 
  mutate(policy = coalesce(phase_1, phase_2, phase_3, stay_home)) %>% 
  group_by(state, county) %>%
  fill(policy, .direction = 'down') %>%
  ungroup() %>%
  mutate(policy = replace_na(policy, 'none_issued')) %>% 
  mutate(policy = as.factor(policy)) %>%
  mutate(policy = fct_relevel(policy, policy_labels)) %>%
  select(-state, -county, -stay_home:-phase_3)


# Add the policy column
dd <- inner_join(dd, policies, by=c('county_fip', 'date'))
# select(d, state, county, date, matches('stay|phase'), policy)

# Sanity Check
policy_check <- dd %>%
  group_by(state, county, county_fip, policy) %>% 
  summarise(start = min(date), stop = max(date), .groups = 'drop') %>%
  group_by(state, county, county_fip) %>%
  mutate(order = row_number(), diff_days = start - lag(stop)) %>%
  ungroup()

# The maximum number of days between policy changes should not exceed 1.
# Otherwise this means we have gaps between policy implementations
stopifnot(max(policy_check$diff_days, na.rm = T) == 1)

# Plot Examples
ex_counties <- c(4013, 49035, 36061, 46099, 2013, 12011)
filter(dd, county_fip %in% ex_counties) %>% 
  ggplot(data = ., aes(x = date, y = confirmed)) +
  geom_line() + 
  facet_wrap(~ state + county, nrow = length(ex_counties), scales = 'free_y') +
  scale_fill_viridis_d() +
  ylab('Cumulative Number of Infections') +
  ggtitle('Example of Policy Implementation Over Time') +
  geom_rect(
    data = filter(policy_check, county_fip %in% ex_counties),
    aes(xmin = start, xmax = stop, ymin = -Inf, ymax = Inf, fill = policy), 
    alpha = 0.25, inherit.aes = FALSE
  )
```

Convenience function:

```{r}
plot_w_policy <- function(df, y = NULL) {
  # NB: this calls out to global 'policy_check'
  stopifnot(inherits(df, 'data.frame')); stopifnot(nrow(df) > 0)
  target <- rlang::sym(y)
  
  ggplot(data = df, aes(x = date, y = !!target)) +
    geom_line() + 
    facet_wrap(~ state + county, scales = 'free_y') +
    scale_fill_viridis_d() +
    ylab(y) +
    geom_rect(
      data = filter(policy_check, county_fip %in% unique(df$county_fip)),
      aes(xmin = start, xmax = stop, ymin = -Inf, ymax = Inf, fill = policy), 
      alpha = 0.25, inherit.aes = FALSE
    )
}
```

---

## Infections

Count the number of _new_ infections over a seven-day period (`target`) in
addition to:  

  - `confirmed_new_cases`: New reported infected cases  
  - `confirmed_new_cases_7day_sum`: New reported cases using a trailing 7 day
     sum  
  - `confirmed_new_cases_7day_sum_lead7`: Leading indicator; number of new
     cases (trailing 7 day sum) in the next 7 days  
  - `confirmed_new_cases_7day_sum_lag7`: Lagging indicator; number of new
     cases (trailing 7 day sum) in the last 7 days  
  - `infection_7day_pct_delta*`: Percentage change in the total number of
     of **new** cases (trailing 7 day sum) over the next week  
  - `infection_momentum*`: Ratio of new confirmed cases in the previous 7 days
     and the total number of infections  
  - `infection_penetration*`: Confirmed number of infections scaled by county
     population  
  
* : a constant of `1` is added to these calculations for numerical stability

```{r warning=F}
dd <- dd %>%
  group_by(state, county) %>%
  mutate(
    confirmed_cum = confirmed,
    confirmed_cum_lag_7 = lag(confirmed_cum, 7),
    target = (confirmed_cum - confirmed_cum_lag_7),
    target_per_capita = target / acs_pop_total, 
    confirmed_new_cases = c(NA, diff(confirmed)), 
    confirmed_new_cases_7day_sum = rollsum(confirmed_new_cases, k=7, na.pad=T, align='right') + 1, 
    confirmed_new_cases_7day_sum_lead7 = lead(confirmed_new_cases_7day_sum, 7),
    confirmed_new_cases_7day_sum_lag7 = lag(confirmed_new_cases_7day_sum, 7),
    infection_7day_pct_delta = confirmed_new_cases_7day_sum_lead7/confirmed_new_cases_7day_sum,
    infection_momentum = confirmed_new_cases_7day_sum_lag7/(confirmed_cum + 1), 
    infection_penetration = (confirmed_cum + 1)/acs_pop_total
  ) %>%
  ungroup() %>%
  filter(!is.na(confirmed_cum_lag_7))
```

### What Does our Target Capture? 

In effect, it is capturing the growing (or shrinking) rate of new cases over
the number of cases from the previous week.
```{r}
filter(dd, county_fip %in%  c(36061, 4013)) %>% 
  select(date, state, county, county_fip, acs_pop_total, target, confirmed_cum) %>%
  mutate(pop = round(acs_pop_total/1e6, digits = 2)) %>%
  mutate(id = sprintf('%s: %s (%sM)', state, county, pop)) %>%
  gather(k, v, target, confirmed_cum) %>% 
  ggplot(data=., aes(x=date, y=v)) + 
  geom_line() + 
  facet_wrap(id ~ k, scales='free_y')
```

### Target Across All States + Counties

Notice the clear number of extreme cases throughout.  
We see what looks like a clear peak in April. _Note_: These figures **are not**
scaled in any way.
```{r}
dd %>% 
  mutate(grp_id = paste(state, county)) %>%
  ggplot(data=., aes(x=date, y=target, group=grp_id)) + 
  ggtitle('State and County Confirmed Cases', 
          'Count of New Infections Relative to Previous Week') +
  geom_line(alpha=0.35)
```

### Alternative Smoothing

What if we defined out target at different lags?
_NB: These have differing *y* scales!_

```{r}
c(7, 14, 21, 28) %>% 
  map_df(~ {
    dd %>% 
      mutate(id_ = paste(state, county)) %>% 
      group_by(id_) %>% 
      arrange(date) %>% 
      transmute(
        date, 
        lag_ = sprintf("Target Lag = %02d", .x), 
        target_ = confirmed - lag(confirmed, .x)
      ) %>% 
      ungroup() %>% 
      na.omit()
  }) %>% 
  ggplot(aes(date, target_, group = id_)) + 
  geom_line(alpha = 0.3) + 
  facet_wrap(~ lag_, scales = "free_y")
```

---

## Mobility 

Since all tracking data is based on a baseline (equal to 0 for that day of the
week) we will assume that locations we cannot interpolate (due to their small 
population size) will have behaved 'as usual'. Although, it's not always the
case. There are a few counties with upwards of 800k people (namely, San
Francisco county). There isn't really much we can do about that.  

The mobility data has two types of missing values:  
  1. sometimes, a particular day might be removed to ensure anonymity. These
     values can be interpolated  
  2. other times, an entire region can be omitted. These are trickier. We can
     either assume a baseline of 0 or impute values based on day of the week
     to preserve weekly seasonality  

```{r, mobility}
mobility_cols <- c(
  'retail_and_recreation', 'grocery_and_pharmacy', 'parks',
  'transit_stations', 'workplaces', 'residential'
)

# Missing Values
select(dd, all_of(mobility_cols)) %>%
  map_dbl(~ mean(is.na(.x))) %>%
  enframe('col', 'pct_na')

split(dd, dd$state) %>%
  map_dfr(.id='state', function(df) {
      select(df, all_of(mobility_cols)) %>%
      map_dbl(~ mean(is.na(.x))) %>%
      enframe('col', 'pct_na')}) %>% 
  ggplot(data=., aes(x=col, y=state, fill=pct_na)) + 
  geom_tile() + 
  scale_fill_viridis_c() + 
  ggtitle("Percentage of Missing Values in Mobility Cols by State")

set.seed(42)
mobility_samps <- sample(unique(dd$county_fip), 6)
filter(dd, county_fip %in% mobility_samps) %>%
  gather(k, v, all_of(mobility_cols)) %>%
  mutate(pop_m = round(acs_pop_total/1e6, digits=2)) %>%
  mutate(id = sprintf('%s: %s\n[%sM]', state, county, pop_m)) %>%
  ggplot(data=., aes(x=date, y=v, color=k)) + 
    geom_line() + 
    ggtitle('Mobility Data for 6 Counties w/o Imputation') +
    facet_wrap(~ id, scales = 'free_y' )
```

We use cascading imputations, starting at the most granular and moving to
the most high-level.

```{r, mobility-impute}
# dplyr::across is nice but it's SLOW compared to mutate_at
dd <- dd %>% 
  mutate(time_dow = factor(wday(date, label=TRUE), ordered=FALSE)) %>%
  mutate(time_wk_yr = week(date)) %>%
  # Interpolate, if possible
  group_by(state, county) %>% 
  mutate_at(vars(all_of(mobility_cols)), ~ na.approx(.x, na.rm=F, maxgap=7)) %>%
  ungroup() %>%
  # Median impute, county x wk_yr x dow
  group_by(county, time_wk_yr, time_dow) %>%
  mutate_at(vars(all_of(mobility_cols)), ~ replace_na(.x, median(.x, na.rm=T))) %>%
  ungroup() %>%
  # Median impute, state x wk_yr x dow
  group_by(state, time_wk_yr, time_dow) %>%
  mutate_at(vars(all_of(mobility_cols)), ~ replace_na(.x, median(.x, na.rm=T))) %>%
  ungroup() %>%
  # Median impute, wk_yr x dow
  group_by(time_wk_yr, time_dow) %>%
  mutate_at(vars(all_of(mobility_cols)), ~ replace_na(.x, median(.x, na.rm=T))) %>%
  ungroup(time_wk_yr) %>%
  # Median impute, dow
  mutate_at(vars(all_of(mobility_cols)), ~ replace_na(.x, median(.x, na.rm=T))) %>%
  ungroup(time_dow) %>%
  # Median impute, global
  mutate_at(vars(all_of(mobility_cols)), ~ replace_na(.x, median(.x, na.rm=T)))
```

```{r}
dd %>%
  filter(county_fip %in% mobility_samps) %>%
  gather(k, v, all_of(mobility_cols)) %>%
  mutate(pop_m = round(acs_pop_total/1e6, digits=2)) %>%
  mutate(id = sprintf('%s: %s\n[%sM]', state, county, pop_m)) %>%
  ggplot(data=., aes(x=date, y=v, color=k)) + 
    geom_line() + 
    facet_wrap(~ id, scales = 'free_y' ) + 
    ggtitle('Mobility Data for 6 Counties with Imputation')
```

In addition to the imputation methods listed above these values will also be
scaled by a factor of 100. 

```{r}
dd <- mutate(dd, across(mobility_cols, ~ .x / 100))
```

---

## Demographics

Four categories of demographics have been collected - age, gender, race,
and income. Broadly speaking, we have pretty good coverage for all
categories with the exception of income. But the total median income
is also available, so we take that in, too.

Rather than using the raw counts we will divide by the population
in order to get the percentage (fraction) of people falling in each bucket.

```{r}
acs_cols <- names(dd)[grepl('acs', names(dd))]
acs_age_cols <- acs_cols[grepl('age', acs_cols)]
acs_gender_cols <- acs_cols[grepl('gender_(?!total)', acs_cols, perl = T)]
acs_race_cols <- acs_cols[grepl('race_(?!total)', acs_cols, perl = T)]

select(dd, all_of(acs_cols)) %>%
  map_dbl(~ mean(is.na(.x))) %>%
  enframe('col', 'pct_na') %>%
  arrange(desc(pct_na))
```

### Ages

Make age buckets that correspond to CDC listings, e.g., 
<https://www.cdc.gov/nchs/nvss/vsrr/covid_weekly/index.htm#AgeAndSex>

```{r, demographics}
dd <- dd %>%
  mutate(
    acs_age_le_24 = (
      acs_age_lt_05 + acs_age_05_09 + acs_age_10_14 + acs_age_15_17 +
      acs_age_18_19 + acs_age_20 + acs_age_21 + acs_age_22_24
    ),
    acs_age_25_34 = acs_age_25_29 + acs_age_30_34,
    acs_age_35_44 = acs_age_35_39 + acs_age_40_44,
    acs_age_45_54 = acs_age_45_49 + acs_age_50_54,
    acs_age_55_64 = acs_age_55_59 + acs_age_60_61 + acs_age_62_64,
    acs_age_65_74 = acs_age_65_66 + acs_age_67_69 + acs_age_70_74,
    acs_age_75_84 = acs_age_75_79 + acs_age_80_84,
    acs_age_85_ge = acs_age_85_up
  ) %>% 
  select(-one_of(acs_age_cols)) %>% 
  mutate(across(starts_with("acs_age"), ~ . / acs_pop_total))
```

### Gender

Male and female proportions are complementary, so only keep female.

```{r, gender}
stopifnot(all(dd$acs_pop_total == dd$acs_gender_total))

dd <- dd %>% 
  select(-acs_gender_male, -acs_gender_total) %>% 
  mutate(acs_gender_female = acs_gender_female / acs_pop_total)
```

### Race

Keep all the individual demographics, but add a total "minority" proportion.

```{r}
stopifnot(all(dd$acs_pop_total == dd$acs_race_total))

dd <- dd %>% 
  mutate(acs_race_minority = 1 - acs_race_white / acs_pop_total) %>% 
  select(-acs_race_total)
```

### Household income

There's a single county without median income:

```{r}
dd %>% 
  distinct(state, county, acs_median_hh_inc_total) %>% 
  filter(is.na(acs_median_hh_inc_total))
```

So, we take a population-weighted median from across the state as a quick imputation:

```{r household-income}
imputed_value <- dd %>% 
  filter(state == "New Mexico") %>% 
  distinct(state, county, acs_pop_total, acs_median_hh_inc_total) %>% 
  na.omit() %>% 
  summarize(
    imputed = sum(acs_pop_total * acs_median_hh_inc_total) / sum(acs_pop_total)
  ) %>% 
  pull(imputed)

dd[dd$state == "New Mexico", ]$acs_median_hh_inc_total <- 
  replace_na(dd[dd$state == "New Mexico", ]$acs_median_hh_inc_total, imputed_value)

stopifnot(all(!is.na(dd$acs_median_hh_inc_total)))
```

```{r}
# Drop unused ACS income cols
dd <- dd %>% select(-starts_with("acs_median_hh"), acs_median_hh_inc_total)
```

---

## Weather

Again, cascading median imputation:

 1. By county, wk_yr, and day of week
 2. By state and wk_yr

```{r, weather}
weather_cols <- c('tmpf_mean', 'dwpf_mean', 'relh_mean')

select(dd, all_of(weather_cols)) %>%
    map_dbl(~ mean(is.na(.x))) %>%
    enframe('col', 'pct_na') %>%
    arrange(desc(pct_na))

# dplyr is not made for this many groups :/
dd <- dd %>% 
  group_by(state, county, time_wk_yr, time_dow) %>% 
  mutate_at(vars(all_of(weather_cols)), ~ replace_na(.x, median(.x, na.rm=TRUE))) %>%
  ungroup() %>%
  group_by(state, time_wk_yr) %>% 
  mutate_at(vars(all_of(weather_cols)), ~ replace_na(.x, median(.x, na.rm=TRUE))) %>%
  ungroup()
```

---

## Economy/Employment

Use the daily number of unemployment claims.

```{r, economy}
econ_cols <- c(
  'labor_force', 
  'unemployed', 
  'county_daily_unemployment_change',
  'county_daily_interp_total_claims'
)

select(dd, all_of(econ_cols)) %>%
    map_dbl(~ mean(is.na(.x))) %>%
    enframe('col', 'pct_na') %>%
    arrange(desc(pct_na))
```

Where are the missing values?
They are always towards the end of the series.

```{r}
dd %>% 
  select(state_code, county_fip, date, all_of(econ_cols)) %>% 
  pivot_longer(cols = -c(state_code:date)) %>% 
  group_by(state_code, county_fip, name) %>% 
  summarize(
    min_dt = min(date),
    max_ok = suppressWarnings(max(date[!is.na(value)])),
    min_na = min(date[is.na(value)]),
    max_na = max(date[is.na(value)]),
    max_dt = max(date),
    .groups = "drop"
  ) %>% 
  filter(min_na < max_ok) %>% 
  nrow()
```

Forward-impute NAs at the county level. There's a single HI county with no labor data,
so we impute that at the daily level as the mean labor force per day 
in the rest of the state.

```{r, economy-impute}
# Standard imputation
dd <- dd %>% 
  group_by(state, county) %>% 
  arrange(state, county, date) %>% 
  fill(labor_force, .direction = "down") %>% 
  fill(unemployed, .direction = "down") %>% 
  ungroup() %>% 
  select(-county_daily_unemployment_change, -county_daily_interp_total_claims)

# Per-day imputation at the state level
econ_impute <- dd %>% 
  select(state, date, acs_pop_total, labor_force, unemployed) %>% 
  na.omit() %>% 
  group_by(state, date) %>% 
  summarize(
    across(c(acs_pop_total, labor_force, unemployed), sum), 
    .groups = "drop"
  ) %>% 
  # NB: this is a statewide fraction, we need to convert it to a total count next
  transmute(
    state, date,
    labor_force_impute = labor_force / acs_pop_total,
    unemployed_impute = unemployed / acs_pop_total
  ) %>% 
  ungroup()

nrow_chk_ <- nrow(dd)
dd <- dd %>% 
  left_join(econ_impute, by = c("state", "date")) %>% 
  # Imputation * county pop = imputed value
  mutate(
    labor_force = if_else(
      is.na(labor_force), 
      acs_pop_total * labor_force_impute, 
      labor_force
    ),
    unemployed = if_else(
      is.na(unemployed), 
      acs_pop_total * unemployed_impute, 
      unemployed
    )
  ) %>% 
  select(-unemployed_impute, -labor_force_impute)
```

Finally, normalize to the county population:

```{r}
dd <- dd %>% 
  group_by(state, county) %>%
  mutate(
    unemployment_rate = unemployed/labor_force, 
    unemployment_penetration = lag(unemployed, 7)/unemployed) %>%
  ungroup() %>% 
  mutate(across(c(labor_force, unemployed), ~ . / acs_pop_total))
```


```{r}
stopifnot(sum(is.na(dd$labor_force)) == 0)
stopifnot(sum(is.na(dd$unemployed)) == 0)
stopifnot(nrow(dd) == nrow_chk_)
# NB: labor force can apparently be bigger than 1 in the _original_ data
stopifnot(all(dd$unemployed <= 1))
```

---

## Testing

Testing data is complete once it begins:

```{r}
cov_cols <- c(
  "cov_pos_tests",
  "cov_total_tests"
)

dd %>% 
  select(state, county, county_fip, date, one_of(cov_cols)) %>% 
  pivot_longer(all_of(cov_cols)) %>% 
  group_by(state, county, county_fip, name) %>% 
  summarize(
    max_na = suppressWarnings(max(date[is.na(value)])),
    min_not_na = suppressWarnings(min(date[!is.na(value)])),
    .groups = "drop"
  ) %>% 
  filter(min_not_na < max_na) %>% 
  summarize(nrow = nrow(.))
```

So impute zeros until the first count:

```{r}
dd <- dd %>% 
  group_by(state, county) %>% 
  # Only keep the columns we've checked
  select(-starts_with("cov_"), all_of(cov_cols)) %>% 
  mutate(across(all_of(cov_cols), replace_na, 0L))

stopifnot(all(!is.na(dd$cov_pos_tests)))
stopifnot(all(!is.na(dd$cov_total_tests)))
```

---

# Final checks and writing

## NA checks

```{r}
sums <- colSums(is.na(dd))
sums[sums > 0]
```

## Write Results

```{r}
readr::write_csv(dd, 'data/abt_prepped.csv')
feather::write_feather(dd, 'data/abt_prepped.feather')
```
